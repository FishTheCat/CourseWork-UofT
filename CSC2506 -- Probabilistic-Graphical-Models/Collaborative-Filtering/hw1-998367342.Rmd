---
output: 
  pdf_document:
    fig_caption: yes
---

---
title: "CSC2506 Assignment 1"
author: "Matthew Scicluna"
date: `r Sys.Date()`
---

#Question 1 and 2
See paper attached at end of document for the answers to these questions.

\pagebreak

#Question 3: Spam classification using logistic regression
```{r setup, include=FALSE, results='hide'}


pckgs = c("glmnet","caret","knitr", "reshape2", "ggplot2")

func <- function(x){
  if(!is.element(x, rownames(installed.packages())))
  {install.packages(x, quiet=TRUE, repos = "http://cran.rstudio.com/")}
}

lapply(pckgs, func)
lapply(pckgs, library, character.only=TRUE)

knitr::opts_chunk$set(cache=TRUE)
```

```{r, echo=FALSE,results='hide', warning=FALSE, cache=TRUE}
train.DF <- read.table('Data/spambase.train.txt', header=FALSE, sep=",")
train.DF[,"V58"] = as.factor(train.DF[,"V58"])
test.DF <- read.table('Data/spambase.test.txt', header=FALSE, sep=",")
test.DF[,"V58"] = as.factor(test.DF[,"V58"])


#make grid finer and smaller
eGrid <- expand.grid(lambda = seq(0.002,0.1,0.001), alpha = rep(0,100) )

fitControl <- trainControl(method = "cv",
                       number = 5,
                       verboseIter = TRUE)

fit <- train(V58 ~.,
             data = train.DF,
             method = "glmnet",
             tuneGrid = eGrid,
             trControl = fitControl,
             family = "binomial")


train.pred <- predict(fit, newdata=train.DF[,1:57])
test.pred <- predict(fit, newdata=test.DF[,1:57])

train.err <- mean(as.factor(train.pred) != train.DF[,58])
test.err <- mean(as.factor(test.pred) != test.DF[,58])

#Normalizing the covariates
train.DF2 = train.DF
test.DF2 = test.DF
train.DF2[,1:57] <- scale(train.DF[,1:57], center=TRUE, scale=TRUE)
test.DF2[,1:57] <- scale(test.DF[,1:57], center=TRUE, scale=TRUE)

fit2 <- train(V58 ~.,
             data = train.DF2,
             method = "glmnet",
             tuneGrid = eGrid,
             trControl = fitControl,
             family = "binomial")

test.pred2 <- predict(fit, newdata=train.DF2[,1:57])
test.pred2 <- predict(fit2, newdata=test.DF2[,1:57])


train.err2 <- mean(as.factor(test.pred) != test.DF[,58])
test.err2 <- mean(as.factor(test.pred2) != test.DF[,58])

#Log trainsforming the covariates
train.DF.log <- train.DF
test.DF.log <- test.DF
train.DF.log[,1:57] <- log(1+train.DF[,1:57])
test.DF.log[,1:57] <- log(1+test.DF[,1:57])

fit3 <- train(V58 ~.,
             data = train.DF.log,
             method = "glmnet",
             tuneGrid = eGrid,
             trControl = fitControl,
             family = "binomial")


train.pred <- predict(fit3, newdata=train.DF.log[,1:57])
test.pred <- predict(fit3, newdata=test.DF.log[,1:57])


train.err3 <- mean(as.factor(train.pred) != train.DF[,58])
test.err3 <- mean(as.factor(test.pred) != test.DF[,58])

#Transforming the covariates into binary variables
train.DF.bin<-train.DF
test.DF.bin <- test.DF
train.DF.bin[train.DF.bin != 0] <- 1
test.DF.bin[test.DF.bin != 0] <- 1

fit4 <- train(V58 ~.,
             data = train.DF.bin,
             method = "glmnet",
             tuneGrid = eGrid,
             trControl = fitControl,
             family = "binomial")

train.pred <- predict(fit4, newdata=train.DF.bin[,1:57])
test.pred <- predict(fit4, newdata=test.DF.bin[,1:57])


train.err4 <- mean(as.factor(train.pred) != train.DF[,58])
test.err4 <- mean(as.factor(test.pred) != test.DF[,58])

ResultTab <- data.frame(Lambda=c(fit$bestTune[1,2],fit2$bestTune[1,2],fit3$bestTune[1,2],fit4$bestTune[1,2]), 
                        Train.err = c(train.err, train.err2, train.err3, train.err4),
                        Test.err = c(test.err,test.err2,test.err3,test.err4) )
  
```

We used the [glmnet](https://cran.r-project.org/web/packages/glmnet/glmnet.pdf) package to get a $l^2$ regularized logistic regression function. We used the \texttt{train} function from the [Caret](https://cran.r-project.org/web/packages/caret/caret.pdf) package to train the regularization parameter (lambda) using 5 fold cross validation.  The regularization parameter was selected from a simple grid search and we only considered values of lambda that were between 0 and 0.1, since practice runs indicated that values > 0.1 produced very innacurate classifiers. We transformed the data in one of four ways and trained the same regularized logistic model and compared the resultant optimal lambda along with its training and test error. The results are presented in table 1.

```{r, echo=FALSE, warning=FALSE, cache=TRUE}
ResultTab2 <- round(ResultTab,3)
rownames(ResultTab2) <- c("No Transformation","Centered Data","Log Transformed Data","Binarized Data")
colnames(ResultTab2) <- c("Lambda","Training Error","Test Error")

knitr::kable(ResultTab2, cap="The best Lambda and training and test errors of each model with different data transformations applied")

DAT <- data.frame(lambda=fit$results[,2],
                  None = fit$results[,3],
                  Centered = fit2$results[1:99,3],
                  LogTransformed = fit3$results[1:99,3],
                  Binarize = fit4$results[1:99,3])
```

In figure 1 we can see the effect of each data transformation of the selection of the optimal lambda. We can see that the accuracy is always highest at small values of lambda and gets progressively worse, justifying our decision to only consider lambda's $\leq 0.1$

```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.cap="Comparing the accuracy of each Lambda between the models with different data transformations applied",fig.height=3}


DAT2 <- melt(DAT,id=("lambda"))
colnames(DAT2)<- c("lambda","Transformation","Accuracy")


ggplot(data=DAT2, aes(lambda, Accuracy))+
geom_point(aes(color=Transformation), size=3)+
  labs(x="Lambda",
       y="Accuracy",
       title="Lambda Values vs Accuracy")

```

We notice that by applying $\log(x_{ij} + 1)$ to each $x_{ij} \in Dataset$ results in the model having the lowest test error. We look at absolute value of each of the weights of the logistic regression model trained on the logarithmically transformed data to see which 5 features are most informative in determining that an email should be marked as spam.


```{r, echo=FALSE, warning=FALSE, cache=TRUE}
ind = as.numeric(rownames(fit3$bestTune))
coeffs = fit3$finalModel$beta[,ind]
smallest = names(sort(abs(coeffs))[1:5])
largest = names(sort(abs(coeffs))[53:57])

dd <- round( c( sort(abs(coeffs))[1:5], sort(abs(coeffs))[53:57] ), 4)

knitr::kable(t(dd), cap="The values of the 5 smallest and largest weights of the logistic model trained on log transformed data")
```

We see that `r smallest` are the weights with the smallest values (and thus they govern the features most likely to indicate non-spam) and `r largest` are the weights with the largest values, and so are most likely to indicate spam.

#Question 4: Collaborative Filtering
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.cap="Assessing the fit of the normal distribution with its maximum likelihood parameters to the movie ratings data. The normal density is plotted in red.", fig.height=3}
mov.df<-read.table("Data/u.data")
ratings<-data.frame( ratings = as.numeric(mov.df[,3]) )


mu <- mean(ratings$ratings)
sigma <- var(ratings$ratings)
y = data.frame( props = prop.table(table(ratings$ratings)))


ggplot(data=ratings, aes(x=ratings))+
  geom_bar(aes(y = ..density..), binwidth=1, color="black", fill="white")+
  stat_function(fun = dnorm, args = list(mu, sigma), fill="red", colour="red", geom="ribbon", alpha=0.2, 
                mapping =    aes(ymin=0,ymax=..y..)) +
  labs(x="Ratings",y="Proportion",title="Fitting a Normal Distribution to the Movie Ratings Data")

```

The log likelihood for a normal distribution is as follows:

   $$\ln\mathcal{L}(\mu,\sigma^2)
     = -\frac{N}{2}\ln(2\pi) - \frac{N}{2}\ln\sigma^2 - \frac{1}{2\sigma^2}\sum_{i=1}^N (x_i-\mu)^2.$$
  
Differentiating the log likelihood and setting this to zero yields the following maximum likelihood estimates:
$\mu^{(mle)}=\frac{1}{N}\sum_{i=1}^N x_i$ and $\sigma^{2 (mle)} = \frac{1}{N} \sum_{i=1}^N (x_i - \overline{x})^2$.

We plotted the empirical distribution of the movie ratings along with the fitted normal distribution superimposed on it. We clearly see that the distribution has a heavy tail, meaning that there are a lot more ratings above 3 than below or equal to it. This is unsurprising since we are analyzing the distribution of user submitted movie ratings. People submitting rakings to these movies would have been willing to watch the movies in the first place-- meaning  that the movies must have had some sort of appeal to them apriori.

We can see that the normal distribution is not a good fit since it has symmetric tails and is unable to capture the skewness of the empirical distribution.

\newpage

We next fit the data to the beta-binomial distribution. This distribution has the following likelihood:

$$\sum_{i=1}^N \ln\mathcal{L}(x_i \mid \theta) = \sum_{i=1}^N \ln {N\choose x_i}\frac{\mathrm{B}(x_i+\alpha,N- x_i+\beta)} {\mathrm{B}(\alpha,\beta)}\!$$

We can derive the method of moments estimates analytically. The estimators are as follows:

$$\alpha^{MoM} =\frac{Nm_1-m_2}{N(\frac{m_2}{m_1}-m_1-1)+m_1}$$

$$\beta^{MoM} =\frac{(N-m_1)(N-\frac{m_2}{m_1})}{N(\frac{m_2}{m_1}-m_1 - 1)+m_1}$$

where $m_1 = \frac{1}{N} \sum_{i=1}^N x_i$ and $m_2 = \frac{1}{N} \sum_{i=1}^N x_i^2$


```{r, warning=FALSE, echo=FALSE, fig.cap="Fitting a beta binomial distribution to the movie ratings data",fig.height=3}

newrats <- ratings$ratings - 1

m1 <-mean(newrats)
m2 <- var(newrats) + m1^2
m2_m1 <- m2/m1
N <- max(newrats)

alpha_hat = (N*m1 - m2)/(N*(m2_m1 - m1 - 1) + m1)
beta_hat = ((N-m1)*(N-m2_m1)) /(N*(m2_m1 - m1 - 1) + m1)

dbetabin <- function(k,Alpha, Beta)
{k = k-1  
return((choose(N,k)*beta((k+Alpha), (N-k+Beta))) / (beta(Alpha, Beta)))}

ggplot(data=ratings, aes(x=ratings))+
  geom_bar(aes(y = ..density..), binwidth=1, color="black", fill="white")+
  stat_function(fun = dbetabin,args = list(Alpha=alpha_hat, Beta=beta_hat), geom="bar",n=5, color="red", fill="red", alpha=0.2)+
  labs(x="Ratings",y="Proportion",title="Fitting a Beta Binomial Distribution to the Movie Ratings Data")
```

After some algebra, we get that $\alpha^{MoM}$ = `r round(alpha_hat,2)` and $\beta^{MoM}$ = `r round(beta_hat,2)`. The corresponding distribution is plotted in figure 3, superimposed to the empirical distribution. We can see that this distribution has a good fit, as it captures much more of the skewness compared to the previous normal fit.

Finally, We fit a normal distribution and a beta binomial distribution to a subset of the data, leaving 20% of the data out as a test set to compare the fit of each model.

\newpage

```{r, warning=FALSE, echo=FALSE, fig.cap="Assessing the fit of both Normal and Beta Binomial models fit to the same training data against the test data",fig.height=3}
training <- newrats[1:(0.8*length(newrats))]
test <- newrats[(0.8*length(newrats)+1):(length(newrats))]

#Fit normal to training data and get log likelihood of test data
mu = mean(training)
sigma = var(training)
ll <- sum(log(dnorm(test, mu, sigma)))

#Fit beta binomial to training data and get log likelihood of test data
newrats <- training

m1 <-mean(newrats)
m2 <- var(newrats) + m1^2
m2_m1 <- m2/m1
N <- max(newrats)

alpha_hat = (N*m1 - m2)/(N*(m2_m1 - m1 - 1) + m1)
beta_hat = ((N-m1)*(N-m2_m1)) /(N*(m2_m1 - m1 - 1) + m1)

ll2 <- sum( log (dbetabin((test+1), alpha_hat, beta_hat)))

#Plot them both together

test2 = test + 1
testdat<-data.frame(test2)
mu2 = mu+1

ggplot(data=testdat, aes(x=test2))+
  geom_bar(aes(y = ..density..), binwidth=1, color="black", fill="white")+
  stat_function(fun = dbetabin,args = list(Alpha=alpha_hat, Beta=beta_hat), geom="bar",n=5, color="red", fill="red", alpha=0.2)+
  stat_function(fun = dnorm, args = list(mu2, sigma), fill="blue", colour="blue", geom="ribbon", alpha=0.2, 
                mapping =    aes(ymin=0,ymax=..y..)) +
  labs(x="Ratings",y="Proportion",title="Fitting a Beta Binomial and Normal Dist. to the Movie Ratings Data")
```

The log likelihood for the Normally fitted model was $`r round(ll)`$ and for the Beta Binomial model it was $`r round(ll2)`$ . Not surprisingly, we see that the data was more likely to come from the Beta Binomial distribution than from a normal one. This confirms our observation from figures 2 and 3 that the Beta Binomial was a better fit to the data.
