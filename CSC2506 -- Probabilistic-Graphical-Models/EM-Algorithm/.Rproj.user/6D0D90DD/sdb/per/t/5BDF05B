{
    "collab_server" : "",
    "contents" : "---\noutput: \n  pdf_document:\n    fig_caption: yes\n    fig_width: 4\n    fig_height: 4\n---\n\n---\ntitle: \"CSC2506 Assignment 3\"\nauthor: \"Matthew Scicluna\"\nheader-includes:\n   - \\usepackage{amsmath}\ndate: `r Sys.Date()`\n---\n\n```{r, echo=FALSE, message=FALSE, results='hide'}\npckgs<-c(\"R.matlab\")\n\nfunc <- function(x){\n  if(!is.element(x, rownames(installed.packages())))\n  {install.packages(x, quiet=TRUE)}\n}\n\nlapply(pckgs, func)\nlapply(pckgs, library, character.only=TRUE)\n\n\n```\n\n```{r, echo=FALSE, warning=FALSE, message=FALSE, cache=FALSE, fig.cap=\"Negative log likelihood of model  trained with different numbers of clusters\"}\n\nsource(\"computeRes.R\")\n\ndata <- readMat(\"Data\\\\a3dataFinal.mat\")\n\n#Make training data\ntrainDF = data.frame(data$train.data)\nIND <- colSums(trainDF!=0)>=200 #Now we need to get rid of columns with less than 200 ratings.\ntrainDF <- trainDF[,IND]\n\n#Make test data\ntestDF = data.frame(data$test.data)\ntestDF <- testDF[,IND] #Now we need to get rid of rows with less than 200 ratings.\n\n#Create EM function\nEM_fun <- function(k, DF, iternum){\n  theta.init <- rep(1/k,k)\n  beta.init <- array( runif(n=(ncol(DF)*k*5)), dim=c(5,ncol(DF),k) )\n  Resp.init <- matrix(NA,nrow=nrow(DF),ncol=k) #Initialize responsibilities\n  llperiter <- matrix(NA,nrow=iternum,1) #Initialize count of lls\n  \n  for (j in 1:k){\n    b1<-beta.init[,,j]\n    b1 <- matrix(b1,nrow=5) / t(matrix(rep(colSums(b1),5),ncol=5))\n    beta.init[,,j] <- b1} #Normalize initial probabilities\n\n  \n  betalist <- as.list(rep(NA,(iternum+1)))\n  thetalist <- as.list(rep(NA,(iternum+1)))\n  resplist <- as.list(rep(NA,(iternum+1)))\n  betalist[[1]] <- beta.init\n  thetalist[[1]] <- theta.init\n  resplist[[1]] <- Resp.init\n  \n  for (iter in 1:iternum){\n  \n  #ESTEP\n  \n    beta.cur <- betalist[[iter]]\n    theta.cur <- thetalist[[iter]]\n    \n    resp.update <- compute_responsibility(DF, beta.cur, theta.cur)\n    \n    resplist[[(iter+1)]] <- resp.update\n\n  #MSTEP\n  beta.update <- array(NA, dim=dim(beta.cur))\n\n  theta.update <- colSums(resp.update) / nrow(resp.update) #Optimize for theta\n  \n  for (i in 1:5) #Optimize for betas\n      {rmat=DF==i\n      beta.update[i,,] <- t(rmat)%*%resp.update}\n  \n  for (j in 1:k){\n    b1<-beta.update[,,j] + 1e-32\n    b1 <- matrix(b1,nrow=5) / t(matrix(rep(colSums(b1),5),ncol=5))\n    beta.update[,,j] <- b1} #Normalize initial probabilities\n  \n    \n    \n  #compute log likelihood\n  ll = compute_ll(DF, beta.update, theta.update, resp.update)\n  \n  \n  paste(\"The Log Likelihood after iteration\", iter,\"is\", ll)\n  llperiter[iter,] = ll\n  betalist[[(iter+1)]] <- beta.update\n  thetalist[[(iter+1)]] <-theta.update\n  \n  if (iter>=3){\n    if (abs(llperiter[iter]-llperiter[(iter-1)]) < 1) #Stopping critereon\n    {return(list(llperiter, theta.update, beta.update,resp.update, thetalist, betalist, resplist) )}\n    }\n  \n  }\n  return(list(llperiter, theta.update, beta.update,resp.update, thetalist, betalist, resplist) )\n  }\n\n\n\nINDX <- sample(1:nrow(trainDF), floor(0.75*nrow(trainDF)))\ntrainDF2 <- trainDF[INDX,]\nvalidDF <- trainDF[-INDX,]\nBigObj <- lapply(1:20, EM_fun, trainDF2, 6)\n\nllpergroup <- rep(NA,20)\nfor (ii in 1:20){\n  llpergroup[ii] <- compute_ll(validDF, \n                              BigObj[[ii]][3][[1]],\n                              unlist(BigObj[[ii]][2]),\n                              compute_responsibility(validDF,BigObj[[ii]][3][[1]],unlist(BigObj[[ii]][2]))\n                                     )}\n\nbarplot(-unlist(llpergroup),xlab=\"Number of Components\", ylab=\"Validation -ve Log Likelihood\", main=\"# Components vs -ve LL\")\n\n```\n\n#Training a Mixture Model for Movie Ratings\nWe trained a multinomial mixture model on the MovieLens data. The dataset consists of ratings of movies (1 to 5) of 843 users of 89 movies. We limited the movies to only ones with at least 200 ratings to avoid overfitting. To import the data we used the $\\texttt{R.matlab}$ package. Information about the package can be found [here](https://cran.r-project.org/web/packages/R.matlab/R.matlab.pdf). We used a custom written EM algorithm to train the model since our data had missing values in it. Our model was similar to the standard multinomial EM model but with indicators added to handle the missing ratings.\n\n##Optimizing the Number of Mixture Components\nWe trained the model varying the number of mixture components on a subset of the training data. We plotted the number of components in each model with the to see log likelihood of the remaining training data (the validation set). Results can be found in figure 1. We found that the model with the best validation log likelihood had 2 clusters. Surprisingly, increasing the number of components beyond this increased the negative validation data log likelihood.\n\n##Consistency of Log Likelihood Across Different Initializations\n```{r, warning=FALSE, echo=FALSE, cache=FALSE, fig.cap=\"Assessing the consistency of the negative log likelihood after several random restarts\"}\nlisty <- list(rep(NA,10))\nfor (i in 1:10)\n  {BigObj2=EM_fun(5,trainDF2,100)\n  x=unlist(BigObj2[[1]])\n  listy[[i]]=min(x[!is.na(x)])}\n\nplot(-unlist(listy),xlab=\"Seperate Run\", ylab=\"-ve Log Likelihood\", main=\"-ve LL Per Run, holding K constant\")\n```\n\nFrom figure 2 We see that the model can vary across different initializations. This is not surprising since the model is supposed to be sensative to the initial values of the betas.\n\n##Expected Complete data log likelihood as a function of EM Iteration\n\nFor the model with 2 components, we plotted the negative log likelihood as a function of the number of iterations. We see that the model decreases monotonically, as is expected. The results are presented in figure 3. We note that we implemented early stopping after 6 interations to avoid overfitting.\n\n```{r, warning=FALSE, echo=FALSE, cache=FALSE, fig.cap=\"Expected complete (negative) data log likelihood as a function of iteration of the EM algorithm\"}\n\nx=-unlist(BigObj[[2]][1])\nplot(x[!is.na(x)],xlab=\"Iteration Number\", ylab=\"-ve Log Likelihood\", main=\"-ve LL Per Iteration\")\n```\n\n##Interpretation of the Mixture Components\n\n```{r, warning=FALSE, echo=FALSE, cache=FALSE}\n\nprofiles<-read.table(\"Data\\\\u.user\", sep=\"|\")\n\nlibrary(R.matlab)\ndata_profiles <- readMat(\"Data\\\\Test_user_indices.mat\")\ndata_profiles2 <- readMat(\"Data\\\\Train_user_indices.mat\")\n\nPeople_Profile <- profiles[data_profiles2[[1]],]\n\nclust.num = 2\n\nresp=compute_responsibility(trainDF[1:842,],BigObj[[clust.num]][3][[1]],unlist(BigObj[[clust.num]][2]))\ncluster.id <- apply(resp, 1, which.max)\n\nPeople_Profile2 <- cbind(People_Profile[,2:4],cluster.id)\n\n#Lets do some analysis\ndatt<-matrix(NA,nrow=6,ncol=clust.num)\n\nfor (mm in 1:clust.num)\n{datt[1:2,mm]<- table(People_Profile2[People_Profile2$cluster.id==mm,][,2])\ndatt[3,mm]<-round(mean(People_Profile2[People_Profile2$cluster.id==mm,1]),1)\ndatt[4:6,mm]<-names(sort(table(People_Profile2[People_Profile2$cluster.id==mm,3]),decreasing=TRUE)[1:3])\n}\n\nrownames(datt)<-c(\"Number of Females\", \"Number of Males\",\"Avg Age\", \"Most Common Occupation\",\"2nd Most Common Occupation\",\"3rd Most Common Occupation\")\ncolnames(datt)<-1:clust.num\n\nknitr::kable(datt, caption=\"Demographic profile of each cluster\")\n```\n\nWe analyzed the demographic composition of the 2 clusters our best model. The data was made available from the MovieLens database. We analyzed number of males and females, average age, and the three most common occupations. We did not find any discernable pattern for any number of clusters. For completeness we present the results in table 1.\n\n##Dirichlet Prior\n\nWe derive EM expressions to train a multinomial mixture model with a Dirichlet prior over the $\\beta_{ vjk}$ parameters. $P(\\beta_{jk} \\mid \\phi_k)= D(\\beta_{jk} \\mid \\phi_k)$\n\nDenote the marginal density of the latent variable as $P(Z_i)=\\theta_k$ and the posterior ratings distribution as $P(R_i \\mid Z_i=k, \\beta) = \\prod_{j}\\prod_{v}\\beta_{vjk}^{[r_{ij}=v]} =Categorical(\\beta_{vjk})$.\n\nFirst we compute the posterior distribution of the latent variable. This amounts to computing the responisibilities in the E step in the EM algorithm.\n\n\\begin{align}\nP(Z_i =k \\mid R_i, \\beta, \\phi, \\theta) &= P(Z_i =k \\mid R_i, \\beta, \\theta) \\\\\n&= \\frac{P(R_i \\mid Z_i=k, \\beta, \\theta)P(Z_i=k \\mid \\theta)}{\\sum_m P(R_i \\mid Z_i=m, \\beta, \\theta)P(Z_i=m \\mid \\theta)} \\\\\n&= \\frac{\\prod_j \\prod_v \\beta_{vjk}^{[r_{ij} = v]} \\theta_k}{\\sum_m \\prod_j \\prod_v \\beta_{vjm}^{[r_{ij} = v]} \\theta_m}\n\\end{align}\n\n\nWe see that this does not affect the responsibilities of the latent variable. We denote the responsibility of cluster k to data point i as $\\gamma_{ik} := P(Z_i = k \\mid R_i, \\theta_k, \\phi_k)$.\n\nWe note that since the Dirichlet distribution is conjugate to the categorical distribution so we can easily compute the joint distribution of the data and the latent variable (Tu, S).\n\n\\begin{align}\nP(Z_i = k,R_i \\mid \\phi, \\theta) &= P(R_i \\mid Z_i = k, \\phi)P(Z_i = k \\mid \\theta) \\\\\n&=P(R_i \\mid Z_i = k, \\beta_{jk}) P(\\beta_{jk} \\mid \\phi_k) P(Z_i = k \\mid \\theta) \\\\\n&=Categorical(\\beta_{vjk}) D(\\beta_{jk} \\mid \\phi_k) \\theta_k \\\\\n&=D( [r_{ij} = v] + \\phi_{vk} - 1)\\theta_k\n\\end{align}\n\nWe now compute $L$, the Expected complete data log likelihood so we can get our $\\beta$ and $\\theta$ updates. Note that the Expected complete data log likelihood is exactly the observed data log likelihood (Marlin, B).\n\n\\begin{align}\nL &= \\sum_{i} \\sum_{k} P(Z_i = k \\mid R_i, \\theta_k, \\phi_k)log P(Z_i = k, R_i \\mid \\theta_k, \\phi_k) \\\\\n&= \\sum_{i} \\sum_{k} \\left( \\sum_{v} [r_{ik}=v]P(Z_i = k \\mid R_i, \\theta_k, \\phi_k) \\right) log P(Z_i = k, R_i \\mid \\theta_k, \\phi_k) \\\\\n&= \\sum_{i} \\sum_{k} \\gamma_{ik} log P(Z_i = k, R_i \\mid \\theta_k, \\phi_k) \\\\\n&= \\sum_{i} \\sum_{k} \\gamma_{ik} \\left( log\\theta_k +  log D([r_{ij} = v] + \\phi_{vk} - 1) \\right) \\\\\n&= \\sum_{i} \\sum_{k} \\gamma_{ik} \\left( log\\theta_k +  \\sum_j \\left( \\sum_v [r_{ij} = v] + \\phi_{vk} -1 \\right) log \\beta_{vjk} -logB(\\phi_k) \\right)\n\\end{align}\n\nTo solve for $\\theta$ we differentiate $L$ and apply a Lagrange multiplier to ensure the contraints are obeyed:\n\n\\begin{align}\n\\frac{\\partial}{\\partial \\theta_k} L + \\lambda \\left( \\sum_k \\theta_k - 1 \\right) &= \\frac{\\sum_i \\gamma_{ik}}{\\theta_k} + \\lambda = 0 \\\\\n&\\Rightarrow \\lambda \\theta_k = \\sum_i \\gamma_{ik} \\\\\n&\\Rightarrow \\theta_k^{MAP} = \\frac{\\sum_i \\gamma_{ik}}{\\sum_m \\sum_i \\gamma_{im}}\n\\end{align}\n\nNote that (15) follows from (14) upon normalizing $\\theta_k$.\n\nTo solve for $\\beta_{vjk}$ we again differentiate $L$ and apply the appropriate Lagrange multiplier:\n\n\\begin{align}\n& \\frac{\\partial}{\\partial \\beta_{vjk}} L + \\lambda \\left( \\sum_v \\beta_{vjk} - 1 \\right) = 0 \\\\\n&\\Rightarrow \\frac{\\partial}{\\partial \\beta_{vjk}} \\sum_{i} \\sum_{k} \\gamma_{ik} \\left( \\sum_j \\left( \\sum_v [r_{ij} = v] + \\phi_{vk} -1 \\right) log \\beta_{vjk} \\right) + \\lambda = 0 \\\\\n&\\Rightarrow \\frac{\\sum_i \\gamma_{ik} [r_{ij} = v] + \\phi_{vk} -1}{\\beta_{vjk}} = \\lambda \\\\\n&\\Rightarrow \\beta_{vjk}^{MAP} = \\frac{1}{\\lambda} \\left(  \\sum_i \\gamma_{ik} [r_{ij} = v] + \\phi_{vk} -1 \\right) \\\\\n&\\Rightarrow \\beta_{vjk}^{MAP} = \\frac{\\sum_i \\gamma_{ik} [r_{ij} = v] + \\phi_{vk} -1}{\\sum_m \\sum_i \\gamma_{ik} [r_{ij} = m] + \\sum_m \\phi_{mk} - M} \n\\end{align}\n\nAgain we see that (20) follows from (19) by normalization of $\\beta_{vjk}$. From this update formula We can see that adding the Dirichlet prior amounts to adding \"pseudo-counts\" to the beta. This is beneficial since it acts as a regularizer and ensures that the computations do not become numerically stable (i.e. ensuring that the denominator doesn't vanish to zero).\n\n\\newpage\n\n##References\n\n[1] Murphy, K. Machine Learning: A Probablistic Perspective. The MIT Press, Cambridge, 2012.\n\n[2] Marlin, B. Collaborative Filtering: A Machine Learning Perspective (Masters Thesis), 2004.\n\n[3] Tu, S. [The Dirichlet-Multinomial and Dirichlet-Categorical models for Bayesian inference](http://www.cs.berkeley.edu/~stephentu/writeups/dirichlet-conjugate-prior.pdf).\n",
    "created" : 1484341770423.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2231263458",
    "id" : "5BDF05B",
    "lastKnownWriteTime" : 1484342409,
    "last_content_update" : 1484342409089,
    "path" : "D:/Projects/EM-Algorithm/CSC2506_Assignment_3.Rmd",
    "project_path" : "CSC2506_Assignment_3.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}